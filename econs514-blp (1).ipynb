{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport scipy.io\nimport statsmodels.api as sm\nfrom statsmodels.sandbox.regression.gmm import IV2SLS\n\nclass BLP(object):\n    '''\n    Jia Yan\n    02/21/2023\n    '''\n    \n    def __init__(self, path, file, ndraws=500, tol_fp=1e-12):\n        self.ndraws = ndraws\n        self.tol_fp = tol_fp\n        data = scipy.io.loadmat(os.path.join(path, file))\n        v_list = ['outshr', 'const', 'mpd', 'air', 'space', 'hpwt', 'price', 'trend']\n        self.nmarkets = data['trend'].max() + 1\n        self.df = data['share']\n        for item in v_list:\n            self.df = np.concatenate([self.df, data[item]], axis=1)\n        self.df = pd.DataFrame(self.df, columns = ['share'] + v_list)\n        \n        self.attributes = ['const', 'mpd', 'air', 'space', 'hpwt', 'price']\n        self.Xmat = self.df[self.attributes].to_numpy()\n        self.attributes_random = ['const', 'mpd', 'air', 'space', 'hpwt']\n        \n        '''\n        Take standard normal draws\n        '''\n        self.draws = np.random.randn(self.nmarkets, self.ndraws, len(self.attributes_random))    \n        \n        '''\n        creat instruments for price: sum of attributes of rival products\n        '''\n        z_list = ['mpd', 'air', 'space', 'hpwt']\n        self.IV_list = ['const', 'mpd', 'air', 'space', 'hpwt'] # the first part of IV are exogenous regressors\n        for var in z_list:\n            name = var + \"_\" + \"z\"\n            self.IV_list.append(name)\n            self.df[name] = self.df.groupby(['trend'])[var].transform(lambda x: x.sum())\n            self.df[name] = self.df[name] - self.df[var]\n        self.Zmat = self.df[self.IV_list].to_numpy()\n        self.weight_mat = np.linalg.inv(np.matmul(np.transpose(self.Zmat), self.Zmat)) # weighting matrix in GMM estimation\n        pz = np.matmul(self.Zmat, self.weight_mat)\n        pz = np.matmul(pz, np.transpose(self.Zmat))\n        self.project_mat = np.matmul(np.transpose(self.Xmat), pz)\n        self.project_mat = np.matmul(self.project_mat, self.Xmat)\n        self.project_mat = np.linalg.inv(self.project_mat)\n        self.project_mat = np.matmul(self.project_mat, np.transpose(self.Xmat))\n        self.project_mat = np.matmul(self.project_mat, pz)\n        \n    def ols(self):\n        '''\n        replicate the first column of table 3\n        '''\n        y = np.log(self.df['share']/self.df['outshr'])\n        #b = np.matmul(np.transpose(self.Xmat), self.Xmat)\n        #b = np.linalg.inv(b)\n        #b = np.matmul(b, np.transpose(self.Xmat))\n        #return np.matmul(b, y)\n        return sm.OLS(y, self.Xmat).fit()\n        \n    def iv(self):\n        '''\n        replicate the second column of table 3\n        '''\n        y = np.log(self.df['share']/self.df['outshr'])\n        #return np.matmul(self.project_mat, y)\n        return IV2SLS(y, self.Xmat, self.Zmat).fit()\n    \n    def market_share(self, mid, delta, xv):\n        draws = self.draws[mid]\n        s = np.zeros(len(delta))\n        for r in range(self.ndraws):\n            w = draws[r]\n            v = np.exp(delta + (w * xv).sum(axis=1))\n            s = s + (v / (1 + np.sum(v)))\n        \n        return (1/self.ndraws) * s \n    \n    def fixed_point(self, pack):\n        mid = pack['mid']\n        df = pack['df']\n        sigmas = pack['sigmas']\n        s0 = df['share'].to_numpy()\n        xv = sigmas * df[self.attributes_random].to_numpy()\n        check = 1.0\n        delta_ini = np.zeros(len(s0))\n        while check > self.tol_fp:\n            delta_new = delta_ini + (np.log(s0) - np.log(self.market_share(mid, delta_ini, xv)))\n            check = abs(np.max(delta_new - delta_ini))\n            delta_ini = delta_new\n        return delta_new\n        \n    def GMM_obj(self,sigmas):\n        \"\"\"\n        sigmas: an 1_D array with the shape (len(self.attributes_random), ), which contains\n        the standard errors of random coefficients\n        \"\"\"\n        df = self.df.copy()\n        v_list = ['share'] + self.attributes_random\n        \n        '''\n        # step 1: solve mean utility (delta_j) from the fixed-point iteration\n        '''\n        df_list = [{'mid': int(mid), 'df': d[v_list], 'sigmas': sigmas} for mid, d in df.groupby(['trend'])]\n        delta_j = tuple(map(self.fixed_point, df_list))\n        delta_j = np.concatenate(delta_j, axis=0) # an array with the shape(2217,)\n        \n        '''\n        step 2: uncover mean part of coefficients (beta_bar) from delta_j, which is equivalent to \n        running an IV estimation using delta_j as the dependent variable\n        '''\n        beta_bar = np.matmul(self.project_mat, delta_j) \n        \n        '''\n        step 3: uncover ommited product attributes (xi_j) from delta_j and beta_bar\n        '''\n        xi_j = delta_j - np.matmul(self.Xmat, beta_bar)\n        \n        '''\n        step 4: interact xi_j with instruments,which include exogenous regressors (veihicles' own\n        exogenous attributes) and instruments for price (sum of attributes of competing products)\n        '''\n        moments = np.matmul(np.transpose(self.Zmat), xi_j) # an array with the shape (m, ), where m is the number of IVs\n        \n        '''\n        step 5: compute the GMM objective function\n        '''\n        f = np.matmul(moments, self.weight_mat)\n        f = np.matmul(f, moments)\n        return f\n    \n    def optimization(self, objfun, para):\n        '''\n        Parameters\n        ----------\n        objfun : a user defined objective function of para\n            \n        para : a 1-D array with the shape (k,), where k is the number of parameters.\n        Returns\n        -------\n        dict\n            A dictionary containing estimation results\n        '''\n        v = opt.minimize(objfun, x0=para, jac=None, method='BFGS', \n                          options={'maxiter': 1000, 'disp': True})  \n        return {'obj':v.fun, \"Coefficients\": v.x}\n\nif __name__ == \"__main__\":\n    blp = BLP(\"/kaggle/input/blp-data/\", \"BLP_data.mat\")\n    pini = np.ones(len(blp.attributes_random)) * 0.2\n    x = blp.GMM_obj(pini)\n    beta_ols = blp.ols()\n    beta_iv = blp.iv()\n    print(beta_ols.summary())\n    print(beta_iv.summary())\n    print(sm.OLS(blp.df['price'], blp.Zmat).fit().summary()) # first-stage regression in IV estimation","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-22T04:16:35.032970Z","iopub.execute_input":"2023-02-22T04:16:35.033409Z","iopub.status.idle":"2023-02-22T04:16:38.693995Z","shell.execute_reply.started":"2023-02-22T04:16:35.033377Z","shell.execute_reply":"2023-02-22T04:16:38.689128Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.387\nModel:                            OLS   Adj. R-squared:                  0.386\nMethod:                 Least Squares   F-statistic:                     279.2\nDate:                Wed, 22 Feb 2023   Prob (F-statistic):          6.52e-232\nTime:                        04:16:38   Log-Likelihood:                -3319.4\nNo. Observations:                2217   AIC:                             6651.\nDf Residuals:                    2211   BIC:                             6685.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -10.0716      0.253    -39.822      0.000     -10.568      -9.576\nx1             0.2650      0.043      6.146      0.000       0.180       0.350\nx2            -0.0343      0.073     -0.472      0.637      -0.177       0.108\nx3             2.3421      0.125     18.707      0.000       2.097       2.588\nx4            -0.1243      0.277     -0.448      0.654      -0.668       0.419\nx5            -0.0886      0.004    -22.014      0.000      -0.097      -0.081\n==============================================================================\nOmnibus:                      158.000   Durbin-Watson:                   1.478\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              205.595\nSkew:                          -0.632   Prob(JB):                     2.27e-45\nKurtosis:                       3.792   Cond. No.                         203.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                          IV2SLS Regression Results                           \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.357\nModel:                         IV2SLS   Adj. R-squared:                  0.356\nMethod:                     Two Stage   F-statistic:                     174.6\n                        Least Squares   Prob (F-statistic):          6.58e-157\nDate:                Wed, 22 Feb 2023                                         \nTime:                        04:16:38                                         \nNo. Observations:                2217                                         \nDf Residuals:                    2211                                         \nDf Model:                           5                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -10.2108      0.269    -37.908      0.000     -10.739      -9.683\nx1             0.3483      0.062      5.577      0.000       0.226       0.471\nx2            -0.4982      0.257     -1.939      0.053      -1.002       0.006\nx3             2.3871      0.130     18.301      0.000       2.131       2.643\nx4            -1.3274      0.698     -1.901      0.057      -2.697       0.042\nx5            -0.0467      0.023     -2.065      0.039      -0.091      -0.002\n==============================================================================\nOmnibus:                      143.801   Durbin-Watson:                   1.442\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              179.610\nSkew:                          -0.611   Prob(JB):                     9.96e-40\nKurtosis:                       3.673   Cond. No.                         203.\n==============================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.578\nModel:                            OLS   Adj. R-squared:                  0.576\nMethod:                 Least Squares   F-statistic:                     377.5\nDate:                Wed, 22 Feb 2023   Prob (F-statistic):               0.00\nTime:                        04:16:38   Log-Likelihood:                -6971.6\nNo. Observations:                2217   AIC:                         1.396e+04\nDf Residuals:                    2208   BIC:                         1.401e+04\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.2975      2.856      0.454      0.650      -4.302       6.897\nx1            -3.0017      0.357     -8.412      0.000      -3.702      -2.302\nx2             9.8939      0.339     29.188      0.000       9.229      10.559\nx3            -1.1414      0.741     -1.541      0.123      -2.594       0.311\nx4            29.4820      1.496     19.712      0.000      26.549      32.415\nx5             0.0145      0.005      2.942      0.003       0.005       0.024\nx6             0.0467      0.018      2.550      0.011       0.011       0.083\nx7             0.0730      0.023      3.149      0.002       0.028       0.118\nx8            -0.2558      0.048     -5.374      0.000      -0.349      -0.162\n==============================================================================\nOmnibus:                     1103.555   Durbin-Watson:                   0.970\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             8772.640\nSkew:                           2.208   Prob(JB):                         0.00\nKurtosis:                      11.687   Cond. No.                     7.66e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 7.66e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n","output_type":"stream"}]}]}