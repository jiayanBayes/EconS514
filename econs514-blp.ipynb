{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport scipy.io\nfrom scipy.optimize import minimize\nfrom itertools import combinations\nimport statsmodels.api as sm\nfrom statsmodels.sandbox.regression.gmm import IV2SLS\n\nclass BLP(object):\n    '''\n    Jia Yan\n    02/21/2023\n    '''\n    \n    def __init__(self, path_data, file_data, path_str_data, file_str_data, ndraws=500, tol_fp=1e-12):\n        '''\n        set parameters\n        '''\n        self.ndraws = ndraws # number of random draws in monte-carlo integration\n        self.tol_fp = tol_fp # convergence tolerance level in nested fixed-point iteration\n        \n        '''\n        construct data \n        '''\n        self.df = self._initialize_data(path_data, file_data, path_str_data, file_str_data)\n        self.nmarkets = int(self.df['trend'].max()) + 1 # a market is a year\n        self.y_fixed = np.log(self.df['share']/self.df['outshr']) # Dependent variable in demand models without random coefficients\n                            \n        '''\n        demand side specification\n        '''\n        attributes = ['const', 'hpwt', 'air', 'mpd', 'space', 'price'] # demand side variables with same order as in the BLP paper\n        self.attributes_random = ['const', 'hpwt', 'air', 'mpd', 'space'] # variables with random coefficients\n        self.Xmat = self.df[attributes].to_numpy() # X-matrix in demand model\n        \n        '''\n        construct demand-side instruments\n        '''\n        self.Zmat_D =  self._initialize_IV_D(self.df) # matrix of demand side instruments\n        self.weight_mat_D = np.linalg.inv(np.matmul(np.transpose(self.Zmat_D), self.Zmat_D)) # weighting matrix in GMM estimation\n        self.project_mat = self._initialize_2sls_D() # inv(X'PX)(X'P) in which P = Z*inv(Z'Z)*Z'  \n        \n        '''\n        supply side specification\n        '''\n        mc = ['const', 'mpg', 'air', 'space', 'hpwt', 'trend'] # marginal cost variables\n        self.Mcmat = self._initialize_mc(mc) # matrix of variables in marginal cost equation\n        wmat_T = np.transpose(self.Mcmat)\n        mom = np.matmul(wmat_T, self.Mcmat)\n        self.PWMAT = np.matmul(np.linalg.inv(mom), wmat_T)\n\n        '''\n        construct supply-side instruments\n        '''\n        self.Zmat_S = self._initialize_IV_S() \n        self.weight_mat_S = np.linalg.inv(np.matmul(np.transpose(self.Zmat_S), self.Zmat_S)) # weighting matrix in GMM estimation\n        \n        ''''\n        Take standard normal draws for approximating integrals in market share and mark-up computations.\n        Better to use halton draws\n        '''\n        self.draws = np.random.randn(self.nmarkets, self.ndraws, len(self.attributes_random))    \n    \n    def _initialize_data(self, path_data, file_data, path_str_data, file_str_data):\n        data = scipy.io.loadmat(os.path.join(path_data, file_data))\n        model_name = scipy.io.loadmat(os.path.join(path_str_data, file_str_data))['model_name']\n        v_list = ['outshr', 'const', 'mpd', 'mpg', 'air', 'space', 'hpwt', 'price', 'trend']\n        df = data['share']\n        for item in v_list:\n            df = np.concatenate([df, data[item]], axis=1)\n        df = pd.DataFrame(df, columns = ['share'] + v_list)\n        df['model_name'] = model_name\n        df['maker'] = df['model_name'].transform(lambda x: x[0:2])\n        df = df.sort_values(by=['trend', 'maker'], ascending=[True, True]) # group products by market and by firm\n        return df\n    \n    def _initialize_IV_D(self, df):\n        # demand side instruments for price: sum of attributes of other products from the same firm and of products from rival firms\n        z_list = ['const'] # creat instruments of price from this list\n        IV_list = ['const', 'mpd', 'air', 'space', 'hpwt'] # the first part of IV are exogenous regressors\n        for var in z_list:\n            name_own = var + \"_\" + \"z\" + \"_\" + \"own\"\n            IV_list.append(name_own)\n            name_rival = var + \"_\" + \"z\" + \"_\" + \"rival\"\n            IV_list.append(name_rival)\n            \n            df[name_own] = df.groupby(['trend', 'maker'])[var].transform(lambda x: x.sum())\n            df[name_own] = df[name_own] - self.df[var]\n            \n            df['junk']= df.groupby(['trend'])[var].transform(lambda x: x.sum()) - self.df[var]\n            df[name_rival] = df['junk'] - df[name_own]\n        \n        return df[IV_list].to_numpy() ## the matrix of demand-side instruments\n    \n    def _initialize_2sls_D(self):\n        pz = np.matmul(self.Zmat_D, self.weight_mat_D)\n        pz = np.matmul(pz, np.transpose(self.Zmat_D))\n        project_mat = np.matmul(np.transpose(self.Xmat), pz)\n        project_mat = np.matmul(project_mat, self.Xmat)\n        project_mat = np.linalg.inv(project_mat)\n        project_mat = np.matmul(project_mat, np.transpose(self.Xmat))\n        return np.matmul(project_mat, pz)\n    \n    def _initialize_mc(self, vlist):\n        df = self.df[vlist].copy()\n        df['mpg'] = np.log(df['mpg'])\n        df['space'] = np.log(df['space'])\n        df['hpwt'] = np.log(df['hpwt'])\n        return df.to_numpy()\n    \n    def _initialize_ols_S(self): \n        mom = np.matmul(np.transpose(self.Mcmat), self.Mcmat)\n        return np.matmul(np.linalg.inv(mom), self.Mcmat)\n    \n    def _initialize_IV_S(self):\n        def util(pair):\n            return np.multiply(pair[0], pair[1])\n        m = self.Mcmat[:, 1:] # remove constant term\n        l = [m[:,i] for i in range(m.shape[1])]\n        pairs = combinations(l, 2)\n        zmat = self.Mcmat # the first part of IVs are exo. regressors\n        interactions = np.array(list(map(util, pairs)))\n        interactions = np.transpose(interactions)\n        return np.hstack((zmat, interactions))\n        \n    def ols(self):\n        '''\n        replicate the first column of table 3\n        '''\n        y = np.log(self.df['share']/self.df['outshr'])\n        return sm.OLS(self.y_fixed, self.Xmat).fit()\n        \n    def iv(self):\n        '''\n        replicate the second column of table 3\n        '''\n        #return np.matmul(self.project_mat, self.y_fixed)\n        return IV2SLS(self.y_fixed, self.Xmat, self.Zmat_D).fit()\n    \n    def hedonic_price(self):\n        '''replicate the third column of table 3'''\n        y = np.log(self.df['price'])\n        return sm.OLS(y, self.Mcmat).fit()\n    \n    def markup_conditional(self, udic):\n        v = np.exp(udic['delta'] + (udic['rdraw'] * udic['xv']).sum(axis=1))\n        s = v / (1 + np.sum(v))\n        cut = 0\n        dmat = []\n        for i in udic['jf']:\n            sg = s[cut:cut+i]\n            dmat.append(np.diag(sg) - np.outer(sg,sg))\n            cut = cut + i\n        return {'dmat': dmat, 'shares': s}\n    \n    def share_conditional(self, udic):\n        v = np.exp(udic['delta'] + (udic['rdraw'] * udic['xv']).sum(axis=1))\n        return v / (1 + np.sum(v))\n        \n    def market_share(self, mid, delta, xv, jf, markup=False):\n        draws = self.draws[mid]\n        inputs = [{'delta': delta, 'xv':xv, 'rdraw':draws[r], 'jf': jf} for r in range(self.ndraws)]\n        if markup is False:\n            out = map(self.share_conditional, inputs)\n            return (1/self.ndraws) * np.sum(list(out), axis=0) \n        else:\n            out = list(map(self.markup_conditional, inputs))\n            d = [i['dmat'] for i in out]\n            s = [i['shares'] for i in out] \n            dmat = [(1/self.ndraws) * item for item in map(sum, zip(*d))]\n            shares = (1/self.ndraws) * np.sum(s, axis=0)\n            return {'dmat': dmat, 'shares':shares}\n       \n    def fixed_point(self, pack):\n        mid = pack['mid']\n        df = pack['df']\n        sigmas = pack['sigmas']\n        s0 = df['share'].to_numpy()\n        xv = sigmas * df[self.attributes_random].to_numpy()\n        jf = [len(g) for _, g in df.groupby(['maker'])]\n        check = 1.0\n        delta_ini = np.zeros(len(s0))\n        while check > self.tol_fp:\n            delta_new = delta_ini + (np.log(s0) - np.log(self.market_share(mid, delta_ini, xv, jf)))\n            check = np.max(abs(delta_new - delta_ini))\n            delta_ini = delta_new\n        \n        '''\n        Calculate markups in a market at current parameter values (delta, sigmas)\n        '''\n        mrkup = self.market_share(mid, delta_new, xv, jf, markup=True)\n        return {'delta': delta_new, 'markup': mrkup} \n    \n    @staticmethod\n    def get_markups(udic):\n        dmat = udic['dmat']\n        shares = udic['shares']\n        alpha = udic['alpha']\n        cut = 0\n        markups = []\n        for d in dmat:\n            svec = shares[cut:cut+d.shape[0]] # shares of products from a firm\n            d = np.linalg.inv(alpha* d)\n            markups.append(np.matmul(d, svec))\n            cut = cut + d.shape[0]\n        return np.concatenate(tuple(markups), axis=0) # a J by 1 vector of product markups \n    \n    def mean_utility(self,sigmas):\n        \"\"\"\n        sigmas: an 1_D array with the shape (len(self.attributes_random), ), which contains\n        the standard errors of random coefficients\n        \"\"\"\n        df = self.df.copy()\n        v_list = ['share', 'maker'] + self.attributes_random\n        \n        '''\n        # step 1: solve mean utility (delta_j) from the fixed-point iteration\n        '''\n        df_list = [{'mid': int(mid), 'df': d[v_list], 'sigmas': sigmas} for mid, d in df.groupby(['trend'])]\n        out = list(map(self.fixed_point, df_list))\n                    \n        '''\n        step 2: uncover mean part of coefficients (beta_bar) from delta_j, which is equivalent to \n        running an IV estimation using delta_j as the dependent variable\n        '''\n        delta_j = tuple([i['delta'] for i in out])\n        delta_j = np.concatenate(delta_j, axis=0) # an array with the shape(2217,)\n        beta_bar = np.matmul(self.project_mat, delta_j) # uncover mean coefficients in demand model\n        \n        '''\n        step 3: uncover ommited product attributes (xi_j) from delta_j and beta_bar\n        '''\n        xi_j = delta_j - np.matmul(self.Xmat, beta_bar)\n        \n        '''\n        step 4: uncover cost-side parameters and cost shocks\n        '''\n        alpha = beta_bar[-1] # the last coefficient in beta_bar is the price coefficient\n        inputs = [{'alpha': alpha, 'dmat': i['markup']['dmat'], 'shares': i['markup']['shares']} for i in out]\n        out = tuple(map(self.get_markups, inputs))\n        markups = np.concatenate(out, axis=0) # an array with the shape(2217,)\n        y_s = self.df['price'] + markups\n        gamma = np.matmul(self.PWMAT, y_s) # parameters of marginal cost equation\n        \n        \"\"\"\n        step 5: uncover omitted product attributes affecting marginal cost\n        \"\"\"\n        omega = y_s - np.matmul(self.Mcmat, gamma)\n        return {'beta_bar': beta_bar, 'xi_j': xi_j, 'gamma': gamma, 'omega': omega} \n    \n    def GMM_obj(self, sigmas):\n        \n        res = self.mean_utility(sigmas)\n        '''\n        step 1: Demand-side moments: interact xi_j with instruments,which include exogenous regressors (veihicles' own\n        exogenous attributes) and instruments for price (sum of attributes of competing products)\n        '''\n        xi_j = res['xi_j']\n        moments_D = np.matmul(np.transpose(self.Zmat_D), xi_j) # an array with the shape (m, ), where m is the number of IVs\n        \n        \"\"\"\n        step 2: Supply-side moments: interact omega with supply-side moments which include exogeneous marginal cost shifters\n        and their interactions\n        \"\"\"\n        omega = res['omega']\n        moments_S = np.matmul(np.transpose(self.Zmat_S), omega)\n        \n        '''\n        step 5: compute the GMM objective function under the assumption that errors in demand and supply equations are independent\n        '''\n        f_D = np.matmul(moments_D, self.weight_mat_D)\n        f_D = np.matmul(f_D, moments_D)\n        f_S = np.matmul(moments_S, self.weight_mat_S)\n        f_S = np.matmul(f_S, moments_S)\n        \n        return (1/len(self.df))* (f_D + f_S)\n    \n    def optimization(self, objfun, para):\n        '''\n        Parameters\n        ----------\n        objfun : a user defined objective function of para\n            \n        para : a 1-D array with the shape (k,), where k is the number of parameters.\n        Returns\n        -------\n        dict\n            A dictionary containing estimation results\n        '''\n        v = opt.minimize(objfun, x0=para, jac=None, method='BFGS', \n                          options={'maxiter': 1000, 'disp': True})  \n        return {'obj':v.fun, \"Coefficients\": v.x}\n\nif __name__ == \"__main__\":\n    \n    blp = BLP(\"/kaggle/input/blp-data/\", \"BLP_data.mat\", \"/kaggle/input/blp-str-data/\", \"BLP_data_str.mat\")\n    pini = np.ones(len(blp.attributes_random)) * 0\n    x = blp.GMM_obj(pini)\n    beta_ols = blp.ols()\n    beta_iv = blp.iv()\n    beta_hedonic= blp.hedonic_price()\n    print(beta_ols.summary())\n    print(beta_iv.summary())\n    print(sm.OLS(blp.df['price'], blp.Zmat_D).fit().summary()) # first-stage regression in IV estimation\n    print(beta_hedonic.summary())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-01T22:22:57.783055Z","iopub.execute_input":"2023-03-01T22:22:57.783847Z","iopub.status.idle":"2023-03-01T22:23:02.760987Z","shell.execute_reply.started":"2023-03-01T22:22:57.783815Z","shell.execute_reply":"2023-03-01T22:23:02.760096Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.387\nModel:                            OLS   Adj. R-squared:                  0.386\nMethod:                 Least Squares   F-statistic:                     279.2\nDate:                Wed, 01 Mar 2023   Prob (F-statistic):          6.52e-232\nTime:                        22:23:02   Log-Likelihood:                -3319.4\nNo. Observations:                2217   AIC:                             6651.\nDf Residuals:                    2211   BIC:                             6685.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -10.0716      0.253    -39.822      0.000     -10.568      -9.576\nx1            -0.1243      0.277     -0.448      0.654      -0.668       0.419\nx2            -0.0343      0.073     -0.472      0.637      -0.177       0.108\nx3             0.2650      0.043      6.146      0.000       0.180       0.350\nx4             2.3421      0.125     18.707      0.000       2.097       2.588\nx5            -0.0886      0.004    -22.014      0.000      -0.097      -0.081\n==============================================================================\nOmnibus:                      158.000   Durbin-Watson:                   1.435\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              205.595\nSkew:                          -0.632   Prob(JB):                     2.27e-45\nKurtosis:                       3.792   Cond. No.                         203.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                          IV2SLS Regression Results                           \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.224\nModel:                         IV2SLS   Adj. R-squared:                  0.222\nMethod:                     Two Stage   F-statistic:                     151.3\n                        Least Squares   Prob (F-statistic):          1.84e-138\nDate:                Wed, 01 Mar 2023                                         \nTime:                        22:23:02                                         \nNo. Observations:                2217                                         \nDf Residuals:                    2211                                         \nDf Model:                           5                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -9.7475      0.302    -32.269      0.000     -10.340      -9.155\nx1             2.6760      0.930      2.878      0.004       0.853       4.499\nx2             1.0455      0.348      3.009      0.003       0.364       1.727\nx3             0.0712      0.078      0.917      0.359      -0.081       0.223\nx4             2.2374      0.145     15.471      0.000       1.954       2.521\nx5            -0.1863      0.031     -6.035      0.000      -0.247      -0.126\n==============================================================================\nOmnibus:                       86.419   Durbin-Watson:                   1.330\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              257.658\nSkew:                          -0.034   Prob(JB):                     1.12e-56\nKurtosis:                       4.669   Cond. No.                         203.\n==============================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.573\nModel:                            OLS   Adj. R-squared:                  0.571\nMethod:                 Least Squares   F-statistic:                     493.3\nDate:                Wed, 01 Mar 2023   Prob (F-statistic):               0.00\nTime:                        22:23:02   Log-Likelihood:                -6984.9\nNo. Observations:                2217   AIC:                         1.398e+04\nDf Residuals:                    2210   BIC:                         1.402e+04\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0456      1.406      0.032      0.974      -2.712       2.803\nx1            -2.9446      0.264    -11.155      0.000      -3.462      -2.427\nx2             9.9354      0.344     28.898      0.000       9.261      10.610\nx3            -0.8568      0.681     -1.259      0.208      -2.192       0.478\nx4            27.3161      1.332     20.505      0.000      24.704      29.929\nx5            -0.0666      0.056     -1.196      0.232      -0.176       0.043\nx6             0.0554      0.008      6.967      0.000       0.040       0.071\n==============================================================================\nOmnibus:                     1112.714   Durbin-Watson:                   0.974\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             8929.792\nSkew:                           2.228   Prob(JB):                         0.00\nKurtosis:                      11.764   Cond. No.                     1.54e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.54e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.656\nModel:                            OLS   Adj. R-squared:                  0.656\nMethod:                 Least Squares   F-statistic:                     844.9\nDate:                Wed, 01 Mar 2023   Prob (F-statistic):               0.00\nTime:                        22:23:02   Log-Likelihood:                -567.01\nNo. Observations:                2217   AIC:                             1146.\nDf Residuals:                    2211   BIC:                             1180.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.7929      0.046     61.188      0.000       2.703       2.882\nx1            -0.4706      0.049     -9.694      0.000      -0.566      -0.375\nx2             0.6798      0.019     36.247      0.000       0.643       0.717\nx3             0.1248      0.063      1.967      0.049       0.000       0.249\nx4             0.5203      0.035     14.833      0.000       0.452       0.589\nx5             0.0128      0.002      8.526      0.000       0.010       0.016\n==============================================================================\nOmnibus:                      533.211   Durbin-Watson:                   1.072\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1412.614\nSkew:                           1.270   Prob(JB):                    1.80e-307\nKurtosis:                       5.974   Cond. No.                         150.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","output_type":"stream"}]},{"cell_type":"code","source":"v = blp.mean_utility(pini)\nv['beta_bar']\nv['gamma']\n","metadata":{"execution":{"iopub.status.busy":"2023-03-01T22:29:55.503489Z","iopub.execute_input":"2023-03-01T22:29:55.503851Z","iopub.status.idle":"2023-03-01T22:30:00.501306Z","shell.execute_reply.started":"2023-03-01T22:29:55.503816Z","shell.execute_reply":"2023-03-01T22:30:00.500283Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array([16.8001972 , -7.96731519, 10.37192072, -3.65111685,  8.38079655,\n        0.15292663])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Notes on the supply side in BLP estimation\nIn BLP paper, there are $F$ firms in a market (year) and each firm offers $J_f$ products. The toatl number of products in a market is therefore $J = \\sum_f J_f$. For convenience in interpretation, we assume that the $J$ products are grouped and ordered by firm. The supply side in BLP paper is the following price equation of a market in matrix form\n\n$$\nP = MC + Markup = W \\gamma - \\Delta(P,X;\\theta)^{-1} \\times S(P,X;\\theta) + \\omega\n$$\n\nwhere \\\n$P$: a $J$ by 1 price vector; \\\n$X$: a $J$ by $K$ matrix of $K$ vehicle attributes excluding price; \\\n$W$: a $J$ by $L$ matrix of $L$ marginal cost shifters; \\\n$\\theta$: demand side parameters; \\\n$\\gamma$: cost side parameters;\\\n$S(P,X;\\theta)$: a $J$ by 1 vector of simulated market shares given current parameter values; \\\n$\\omega$: a $J$ by 1 vector of supply-side shocks; \n$\\Delta(P,X;\\theta)$: a $J$ by $J$ Jacobian matrix defined as \n\\begin{equation}\n\\Delta(P,X;\\theta)=diag(\\Delta_1(P,X;\\theta), \\Delta_2(P,X;\\theta), \\cdots, \\Delta_F(P,X;\\theta)),\n\\end{equation}\nin which $diag()$ denotes the operator of a diagonal matrix and a block in the blocked diagonal matrix $\\Delta_f(P,X;\\theta)$ takes the fowllowing form: \n<br>\n\n$$\n\\Delta_f(P,X;\\theta) = \n\\begin{bmatrix}\n \\frac{\\partial s_1(P,X;\\theta)}{\\partial p_1} & \\frac{\\partial s_2(P,X;\\theta)}{\\partial p_1} & \\cdots & \\frac{\\partial s_J(P,X;\\theta)}{\\partial p_1}\\\\\n\\frac{\\partial s_1(P,X;\\theta)}{\\partial p_2} & \\frac{\\partial s_2(P,X;\\theta)}{\\partial p_2} & \\cdots & \\frac{\\partial s_J(P,X;\\theta)}{\\partial p_2} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n\\frac{\\partial s_1(P,X;\\theta)}{\\partial p_{J_f}} & \\frac{\\partial s_2(P,X;\\theta)}{\\partial p_{J_f}} & \\cdots & \\frac{\\partial s_J(P,X;\\theta)}{\\partial p_{J_f}}\n\\end{bmatrix}\n $$\n <br>\n We can derive analytically the partial derivatives in the above Jacobian matrix. Remember that\n\\begin{equation}\ns_j(P,X;\\theta)= \\int_{V_i}\\frac{exp(X_j \\bar{B} + \\alpha p_j + X_jV_i)}{1+\\sum_{j'}exp(X_{j'} \\bar{B} + \\alpha p_{j'} + X_{j'}V_i)} f(V_i;\\Sigma)dV_i =  \\int_{V_i}s_j(P,X;\\theta|V_i)f(V_i;\\Sigma)dV_i\n\\end{equation}\n\nwhere $f(V_i;\\Sigma)$ is a joint normal density function with zero means and a diaganol variance-covariance matrix $\\Sigma = diag(\\sigma_1,\\sigma_2,\\cdots,\\sigma_K)$. Using Monte-Carlo integration, we have\n\n\\begin{equation}\n\\frac{\\partial s_j(P,X;\\theta)}{\\partial p_{k}}=\\int_{V_i}\\frac{\\partial s_j(P,X;\\theta|V_i)}{\\partial p_{k}}\nf(V_i;\\Sigma)dV_i \\approx\n    \\begin{cases}\n        \\alpha R^{-1} \\sum_{r=1}^{R} s_j(P,X;\\theta|V_i^r) \\times (1 - s_j(P,X;\\theta|V_i^r)) & \\text{if} j=k \\\\\n        -\\alpha R^{-1} \\sum_{r=1}^{R} s_j(P,X;\\theta|V_i^r)\\times s_k(P,X;\\theta|V_i^r) & \\text{if} j \\neq k        \n    \\end{cases}\n\\end{equation}\n","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"g = sm.OLS(blp.df['price'], blp.Mcmat).fit()\ng.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-01T22:29:28.929857Z","iopub.execute_input":"2023-03-01T22:29:28.930215Z","iopub.status.idle":"2023-03-01T22:29:28.968443Z","shell.execute_reply.started":"2023-03-01T22:29:28.930187Z","shell.execute_reply":"2023-03-01T22:29:28.967621Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.540\nModel:                            OLS   Adj. R-squared:                  0.539\nMethod:                 Least Squares   F-statistic:                     518.9\nDate:                Wed, 01 Mar 2023   Prob (F-statistic):               0.00\nTime:                        22:29:28   Log-Likelihood:                -7066.4\nNo. Observations:                2217   AIC:                         1.414e+04\nDf Residuals:                    2211   BIC:                         1.418e+04\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         22.1816      0.856     25.906      0.000      20.503      23.861\nx1            -7.9559      0.911     -8.736      0.000      -9.742      -6.170\nx2            10.3484      0.352     29.417      0.000       9.659      11.038\nx3            -3.5793      1.190     -3.007      0.003      -5.913      -1.245\nx4             8.3805      0.658     12.735      0.000       7.090       9.671\nx5             0.1528      0.028      5.412      0.000       0.097       0.208\n==============================================================================\nOmnibus:                     1156.789   Durbin-Watson:                   0.946\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             9328.649\nSkew:                           2.341   Prob(JB):                         0.00\nKurtosis:                      11.891   Cond. No.                         150.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"","text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.540</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.539</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   518.9</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Wed, 01 Mar 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n</tr>\n<tr>\n  <th>Time:</th>                 <td>22:29:28</td>     <th>  Log-Likelihood:    </th> <td> -7066.4</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>  2217</td>      <th>  AIC:               </th> <td>1.414e+04</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>  2211</td>      <th>  BIC:               </th> <td>1.418e+04</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td>   22.1816</td> <td>    0.856</td> <td>   25.906</td> <td> 0.000</td> <td>   20.503</td> <td>   23.861</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>   -7.9559</td> <td>    0.911</td> <td>   -8.736</td> <td> 0.000</td> <td>   -9.742</td> <td>   -6.170</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>   10.3484</td> <td>    0.352</td> <td>   29.417</td> <td> 0.000</td> <td>    9.659</td> <td>   11.038</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>   -3.5793</td> <td>    1.190</td> <td>   -3.007</td> <td> 0.003</td> <td>   -5.913</td> <td>   -1.245</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>    8.3805</td> <td>    0.658</td> <td>   12.735</td> <td> 0.000</td> <td>    7.090</td> <td>    9.671</td>\n</tr>\n<tr>\n  <th>x5</th>    <td>    0.1528</td> <td>    0.028</td> <td>    5.412</td> <td> 0.000</td> <td>    0.097</td> <td>    0.208</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>1156.789</td> <th>  Durbin-Watson:     </th> <td>   0.946</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>9328.649</td>\n</tr>\n<tr>\n  <th>Skew:</th>           <td> 2.341</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>       <td>11.891</td>  <th>  Cond. No.          </th> <td>    150.</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."},"metadata":{}}]}]}